{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "REzjr490GdJA"
      },
      "source": [
        "Prepare Main project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_google_colab = False\n",
        "first_run = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfCbuSXiFVei",
        "outputId": "1c58e0ba-e540-459d-da89-e7b3b1478dea"
      },
      "outputs": [],
      "source": [
        "# Clone main project\n",
        "if(use_google_colab and first_run):\n",
        "    print(\"clone main project\")\n",
        "    !git clone \"https://github.com/albirrkarim/post-processing-segment-anything-model.git\"\n",
        "    !cd /content/post-processing-segment-anything-model && mv * \"../\"\n",
        "    !rm -rf \"post-processing-segment-anything-model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(use_google_colab and first_run):\n",
        "    print(\"Prepare folder\")\n",
        "    !mkdir precompute\n",
        "    !mkdir output\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XkEV-sypGVs8"
      },
      "source": [
        "Prepare Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q66mAgmAFHcg",
        "outputId": "54a66cb0-a166-40ea-abd5-5ee325b9a5a7"
      },
      "outputs": [],
      "source": [
        "# Clone all algorithm sam, matteformer, aim, inspyrenet\n",
        "\n",
        "if(use_google_colab and first_run):\n",
        "    print(\"clone all algorithm sam, matteformer, aim, inspyrenet\")\n",
        "\n",
        "    # !cd \"algorithms\" && git clone https://github.com/webtoon/matteformer.git\n",
        "\n",
        "    !cd \"algorithms\" && git clone https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "    # !cd \"algorithms\" && git clone https://github.com/JizhiziLi/AIM.git\n",
        "\n",
        "    !pip install transparent-background\n",
        "\n",
        "    !pip install \"algorithms/segment-anything\"\n",
        "\n",
        "    !pip install pymatting"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KfOq5sA5GiL3"
      },
      "source": [
        "Prepare Models (.pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(use_google_colab):\n",
        "    MODEL_DIR = \"models/\"\n",
        "    AIM_NET_MODEL_PATH = MODEL_DIR+\"aimnet_pretrained_matting.pth\"\n",
        "    RESNET_MODEL_PATH = MODEL_DIR+\"r34mp_pretrained_imagenet.pth.tar\"\n",
        "    MATTEFORMER_MODEL_PATH = MODEL_DIR+\"matteformer.pth\"\n",
        "    INSPYRENET_MODEL_PATH = MODEL_DIR+\"InSPyReNet_SwinB_DIS5K.pth\"\n",
        "else:\n",
        "    MODEL_DIR = \"/Users/susanto/Documents/Proyek/best-remove-background/models/\"\n",
        "    AIM_NET_MODEL_PATH = MODEL_DIR+\"aimnet_pretrained_matting.pth\"\n",
        "    RESNET_MODEL_PATH = MODEL_DIR+\"r34mp_pretrained_imagenet.pth.tar\"\n",
        "    MATTEFORMER_MODEL_PATH = MODEL_DIR+\"matteformer_image_matting.pth\"\n",
        "    INSPYRENET_MODEL_PATH = MODEL_DIR+\"InSPyReNet_SwinB_Large.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "QW8dDpXdFHcj",
        "outputId": "c731c697-684b-49bc-8260-999df10ce5e0"
      },
      "outputs": [],
      "source": [
        "# download all models .pth\n",
        "import gdown\n",
        "\n",
        "if (use_google_colab and first_run):\n",
        "    print(\"Download all models\")\n",
        "    !mkdir \"models\"\n",
        "\n",
        "    gdown.download(\n",
        "        \"https://drive.google.com/uc?id=1XBa2oc0yW1WTKOJ0ECr_EK-qIQj4rXIt&export=download\",\n",
        "        output=\"datasets/aim-500.zip\")\n",
        "    \n",
        "    !unzip -q \"datasets/aim-500.zip\" -d \"datasets\"\n",
        "\n",
        "    # AIM NET\n",
        "    # aimnet_pretrained_matting.pth\n",
        "    gdown.download(\n",
        "        \"https://drive.google.com/uc?export=download&id=16dd1FGMcsMTqR6EfD2T9mtRmPwxnY0zs\",\n",
        "        output=AIM_NET_MODEL_PATH)\n",
        "\n",
        "    # r34mp_pretrained_imagenet.pth.tar\n",
        "    gdown.download(\"https://drive.google.com/uc?export=download&id=18Pt-klsbkiyonMdGi6dytExQEjzBnHwY\",\n",
        "                   output=RESNET_MODEL_PATH)\n",
        "\n",
        "    # Matteformer\n",
        "    gdown.download(\"https://drive.google.com/u/0/uc?id=1AU7uM1dtYjEhtOa_9OGfoQUE-tmW9mX5&export=download\",\n",
        "                   output=MATTEFORMER_MODEL_PATH)\n",
        "\n",
        "    # Inspyrenet\n",
        "    gdown.download(\"https://drive.google.com/u/0/uc?id=1aCxHMbhvj8ah77jXVgqvqImQA_Y0G-Yg&export=download\",\n",
        "                   output=INSPYRENET_MODEL_PATH)\n",
        "\n",
        "    first_run = False\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import all make model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wGBLW54NFHck"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "sys.path.append('driver')\n",
        "sys.path.append('algorithms/AIM/core')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AIM\n",
        "from aim import makeAIMNetModel, inference_img as AIMNET_Predictor\n",
        "from aim import calculate_sad_mse_mad_whole_img,compute_connectivity_loss_whole_image,compute_gradient_whole_image\n",
        "\n",
        "aim_model = makeAIMNetModel(device=DEVICE,model_path=AIM_NET_MODEL_PATH,res_net_model_path=RESNET_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B /Users/susanto/Documents/Proyek/best-remove-background/models/InSPyReNet_SwinB_Large.pth\n",
            "Settings -> Mode=fast, Device=cpu, Torchscript=enabled\n"
          ]
        }
      ],
      "source": [
        "# Inspyrenet\n",
        "\n",
        "from transparent_background import Remover\n",
        "\n",
        "InSPyReNet_remover = Remover(\n",
        "    fast=True, jit=True, device=DEVICE, ckpt=INSPYRENET_MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matteformer\n",
        "sys.path.append('algorithms/matteformer')\n",
        "\n",
        "import networks as matteformer_networks\n",
        "import utils_matteformer as utils\n",
        "from inference import generator_tensor_dict, single_inference\n",
        "\n",
        "\n",
        "def makeMatteFormerModel(device='cpu'):\n",
        "    checkpoint_path = MATTEFORMER_MODEL_PATH\n",
        "\n",
        "    # build model\n",
        "    model = matteformer_networks.get_generator(is_train=False)\n",
        "    # model.cpu()\n",
        "\n",
        "    # load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
        "\n",
        "    model.load_state_dict(utils.remove_prefix_state_dict(\n",
        "        checkpoint['state_dict']), strict=True)\n",
        "\n",
        "    model.to(device)\n",
        "    model = model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "matteformer_model = makeMatteFormerModel(DEVICE)\n",
        "\n",
        "def ImageMattingMatteFormer(image_path, trimap_path):\n",
        "    image_dict = generator_tensor_dict(image_path, trimap_path)\n",
        "    alpha_pred = single_inference(matteformer_model, image_dict, device=DEVICE)\n",
        "\n",
        "    return alpha_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
          ]
        }
      ],
      "source": [
        "sys.path.append('helper')\n",
        "\n",
        "from helper import *\n",
        "\n",
        "from post_helper import *\n",
        "\n",
        "from sam_helper import *\n",
        "\n",
        "from evaluation_helper import *\n",
        "\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import datetime\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rgbToAlpha2D(rgb_gray_scale):\n",
        "    # # Convert the image to grayscale\n",
        "    # gray_image = np.mean(rgb_gray_scale, axis=2)\n",
        "\n",
        "    # # Normalize the pixel values between 0 and 1\n",
        "    # normalized_image = gray_image / np.max(gray_image)\n",
        "\n",
        "    # # Scale the values to the desired range (0-255)\n",
        "    # scaled_image = (normalized_image * 255).astype(np.uint8)\n",
        "\n",
        "    # return scaled_image\n",
        "    alpha = np.array(rgb_gray_scale)\n",
        "    alpha = alpha[:, :, 0] if alpha.ndim > 2 else alpha\n",
        "\n",
        "    return alpha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getMSEandSAD(rgba_img, alpha_img):\n",
        "    # mse = getMSE(rgba_img, alpha_img)\n",
        "    # sad = getSAD(rgba_img, alpha_img)\n",
        "\n",
        "    predict = rgba_img[:, :, 3]\n",
        "\n",
        "    predict = predict / 255\n",
        "    alpha_img = alpha_img / 255\n",
        "\n",
        "    sad, mse, mad = calculate_sad_mse_mad_whole_img(predict, alpha_img)\n",
        "    conn = compute_connectivity_loss_whole_image(predict, alpha_img)\n",
        "    grad = compute_gradient_whole_image(predict, alpha_img)\n",
        "\n",
        "    return {\n",
        "        \"sad\": sad,\n",
        "        \"mse\": mse,\n",
        "        \"mad\": mad,\n",
        "        \"conn\": conn,\n",
        "        \"grad\": grad\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crop_image_with_bounding_box(img, bounding_box):\n",
        "    x1, y1, x2, y2 = bounding_box\n",
        "    cropped_image = img[y1:y2, x1:x2,]\n",
        "    return cropped_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getImage(name, folder):\n",
        "    return np.asarray(Image.open(\"datasets/AIM-500/\"+folder+\"/\"+name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cropUnusedBlankPixelExtended(myImage):\n",
        "    # https://stackoverflow.com/a/53829086\n",
        "    if isinstance(myImage, Image.Image):\n",
        "        PIL_image = myImage\n",
        "    else:\n",
        "        PIL_image = Image.fromarray(myImage.astype('uint8'), 'RGBA')\n",
        "\n",
        "    bounding = PIL_image.getbbox()\n",
        "\n",
        "    bounding = extendBoundaries(np.asarray(myImage), bounding, 50)\n",
        "\n",
        "    if (bounding != None):\n",
        "        PIL_image = PIL_image.crop(bounding)\n",
        "    else:\n",
        "        width, height = PIL_image.size\n",
        "        bounding = (0, 0, width, height)\n",
        "\n",
        "    return [PIL_image, np.array(bounding).astype(int)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def addTheRestMask(masks):\n",
        "    mask = masks[0]['segmentation']\n",
        "\n",
        "    for i in range(0, len(masks)):\n",
        "        # Apply Segment Mask\n",
        "        segmentation = masks[i]['segmentation']\n",
        "        mask = np.logical_or(mask, segmentation)\n",
        "\n",
        "    inverted_mask = mask ^ True\n",
        "\n",
        "    masks.append({\n",
        "        \"segmentation\": inverted_mask\n",
        "    })\n",
        "\n",
        "    return masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def findSamObjectMaskThatMatchDataset(file_name, masks, threshold=0.5):\n",
        "    # if the iou between sam and the original is < 0.5\n",
        "    # it mean sam detect false object from the original object in dataset.\n",
        "    # then don't do post processing because the dataset AIM-500 is focused on one object in the picture\n",
        "\n",
        "    file_name_origin = os.path.splitext(file_name)[0]\n",
        "    maximal = 0\n",
        "    maximal_index = 0\n",
        "\n",
        "    # Evaluate ori vs SAM\n",
        "    origin_img = getImage(file_name_origin+\".png\", \"mask\")\n",
        "    origin_mask = origin_img[:, :,\n",
        "                             0] > 0 if origin_img.ndim > 2 else origin_img > 0\n",
        "\n",
        "    for index in range(0, len(masks)):\n",
        "        sam_mask = masks[index][\"segmentation\"]\n",
        "\n",
        "        # Evaluate SAM before post processing\n",
        "        iou = getIoU(origin_mask, sam_mask)\n",
        "\n",
        "        if (iou > maximal):\n",
        "            maximal = iou\n",
        "            maximal_index = index\n",
        "\n",
        "    # add more mask if the iou is increased\n",
        "    basic_mask = masks[maximal_index][\"segmentation\"]\n",
        "    current_iou = maximal\n",
        "\n",
        "    for index in range(0, len(masks)):\n",
        "        sam_mask = masks[index][\"segmentation\"]\n",
        "\n",
        "        result = np.logical_or(sam_mask, basic_mask)\n",
        "\n",
        "        # Evaluate SAM before post processing\n",
        "        iou = getIoU(origin_mask, result)\n",
        "\n",
        "        if (iou > current_iou):\n",
        "            current_iou = iou\n",
        "            basic_mask = result\n",
        "\n",
        "    return basic_mask\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "CACHE=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def useCache(IDX_IMG, pipeline_name, value=None):\n",
        "    if (not CACHE):\n",
        "        return None\n",
        "\n",
        "    cache_path = \"precompute/\"+IDX_IMG+\"_\"+pipeline_name+\".pickle\"\n",
        "    if (value == None):\n",
        "        if (os.path.exists(cache_path)):\n",
        "            # Open the pickle file\n",
        "            with open(cache_path, \"rb\") as file:\n",
        "                loaded_dict = pickle.load(file)\n",
        "            return loaded_dict\n",
        "        return None\n",
        "    else:\n",
        "        # save cache\n",
        "        # Save the dictionary to the generated filename\n",
        "        with open(cache_path, \"wb\") as file:\n",
        "            pickle.dump(value, file)\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def AIM_Pipeline(IDX_IMG, PIL_image, dilated_sam_mask, croped, blank_image, bounding, origin_alpha, origin_mask):\n",
        "    cache = useCache(IDX_IMG, \"aim\")\n",
        "\n",
        "    if (cache == None):\n",
        "        start_time = time.time()\n",
        "\n",
        "        predict = AIMNET_Predictor(aim_model,np.asarray(PIL_image),device=DEVICE)\n",
        "\n",
        "        predict = (predict * 255).astype(np.uint8)\n",
        "\n",
        "        predict[dilated_sam_mask == False] = 0\n",
        "\n",
        "        applied_alpha_aim = croped.copy()\n",
        "        applied_alpha_aim[:, :, 3] = predict\n",
        "\n",
        "        # back to original size\n",
        "        a = blank_image.copy()\n",
        "        alpha_aim_original_size = patchToBoundingBox(\n",
        "            a, bounding, applied_alpha_aim)\n",
        "\n",
        "        Image.fromarray(alpha_aim_original_size).save(\n",
        "            IDX_IMG+\"_aim.png\")\n",
        "\n",
        "        # Evaluate SAM before post processing\n",
        "        AIM_result_origin_size_mask = alpha_aim_original_size[:, :, 3] > 0\n",
        "\n",
        "        res = getMSEandSAD(alpha_aim_original_size, origin_alpha)\n",
        "        res[\"iou\"] = getIoU(origin_mask, AIM_result_origin_size_mask)\n",
        "        res[\"time\"] = time.time() - start_time\n",
        "\n",
        "        useCache(IDX_IMG, \"aim\", [predict, res])\n",
        "    else:\n",
        "        predict, res = cache\n",
        "\n",
        "    return predict, res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Matteformer_Pipeline(IDX_IMG, name_before, JPG_PATH, croped, blank_image, bounding, origin_alpha, origin_mask):\n",
        "    cache = useCache(IDX_IMG, \"matteformer_\"+name_before)\n",
        "\n",
        "    if (cache == None):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Image Matting: MatteFormer\n",
        "        alpha_pred = ImageMattingMatteFormer(JPG_PATH, IDX_IMG+\"_trimap_\"+name_before+\".jpg\")\n",
        "\n",
        "        croped_a = croped.copy()\n",
        "        croped_a[:, :, 3] = alpha_pred\n",
        "\n",
        "        # Back to original size\n",
        "        a = blank_image.copy()\n",
        "        croped_a = patchToBoundingBox(a, bounding, croped_a)\n",
        "\n",
        "        Image.fromarray(croped_a).save(\n",
        "            IDX_IMG+\"_\"+name_before+\"_matteformer.png\")\n",
        "\n",
        "        MatteFormer_result_mask = croped_a[:, :, 3] > 0\n",
        "\n",
        "        res = getMSEandSAD(croped_a, origin_alpha)\n",
        "        res[\"iou\"] = getIoU(origin_mask, MatteFormer_result_mask)\n",
        "        res[\"time\"] = time.time() - start_time\n",
        "\n",
        "        useCache(IDX_IMG, \"matteformer_\"+name_before, res)\n",
        "    else:\n",
        "        res = cache\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def InSpyReNet_Pipeline(IDX_IMG, JPG_PATH, dilated_sam_mask, blank_image, bounding, origin_alpha, origin_mask):\n",
        "    cache = useCache(IDX_IMG, \"InSpyReNet\")\n",
        "\n",
        "    if (cache == None):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        PIL_image = Image.open(JPG_PATH)\n",
        "        InSpyReNet_result = InSPyReNet_remover.process(PIL_image)\n",
        "\n",
        "        # Remove all region  outside dilated mask\n",
        "        InSpyReNet_result[dilated_sam_mask == False] = [0, 0, 0, 0]\n",
        "\n",
        "        # Back to original size\n",
        "        a = blank_image.copy()\n",
        "        InSpyReNet_result_origin_size = patchToBoundingBox(\n",
        "            a, bounding, InSpyReNet_result)\n",
        "\n",
        "        Image.fromarray(InSpyReNet_result_origin_size).save(\n",
        "            IDX_IMG+\"_InSpy.png\")\n",
        "\n",
        "        InSpyReNet_result_origin_size_mask = InSpyReNet_result_origin_size[:, :, 3] > 0\n",
        "\n",
        "        res = getMSEandSAD(InSpyReNet_result_origin_size, origin_alpha)\n",
        "        res[\"iou\"] = getIoU(\n",
        "            origin_mask, InSpyReNet_result_origin_size_mask)\n",
        "\n",
        "        res[\"time\"] = time.time() - start_time\n",
        "\n",
        "        useCache(IDX_IMG, \"InSpyReNet\", [InSpyReNet_result, res])\n",
        "    else:\n",
        "        InSpyReNet_result, res = cache\n",
        "\n",
        "    return InSpyReNet_result, res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def makeTrimapPlusAIM(img, aim_trimap, s_size=2, name=\"example\"):\n",
        "    # Step 1: Select Only Main Visible Area\n",
        "    main_area_mask = img[:, :, 3] > 170\n",
        "    # plt.imshow(main_area_mask)\n",
        "\n",
        "    # Step 2: Detect Edges (For nets rope, etc)\n",
        "    # limit edge detection to visible area only\n",
        "    visible_area = img.copy()\n",
        "    visible_area[img[:, :, 3] < 200] = [0, 0, 0, 0]\n",
        "    PIL_image, bounding = cropUnusedBlankPixel(visible_area)\n",
        "    x, y, x1, y1 = bounding\n",
        "    croped_img = img[y:y1, x:x1]\n",
        "\n",
        "    # Convert RGBA image to grayscale\n",
        "    gray_image = cv2.cvtColor(\n",
        "        croped_img.astype(np.uint8), cv2.COLOR_RGBA2GRAY)\n",
        "\n",
        "    # Apply Canny edge detection\n",
        "    edges = cv2.Canny(gray_image, threshold1=100, threshold2=200)\n",
        "    threshold = 0  # Set the threshold value\n",
        "    edges_mask = edges > threshold\n",
        "\n",
        "    # Back to original size\n",
        "    blank_img = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
        "    blank_img = blank_img != 0\n",
        "    edges_mask = patchToBoundingBox(blank_img, bounding, edges_mask)\n",
        "\n",
        "    # Remove outside visible_area\n",
        "    visible_area = img[:, :, 3] > 20\n",
        "    edges_mask = np.logical_and(edges_mask, visible_area)\n",
        "    # plt.imshow(edges_mask)\n",
        "\n",
        "    # Step 3: Perform Closing Into Edge region\n",
        "    dilated = binary_dilation(edges_mask, structure=np.ones((s_size, s_size)))\n",
        "    closed_edge_mask = binary_erosion(\n",
        "        dilated, structure=np.ones((s_size, s_size)))\n",
        "\n",
        "    # plt.imshow(dilated)\n",
        "\n",
        "    # Step 4: main_area_mask + closed_edge_mask\n",
        "    fore_mask = np.logical_or(main_area_mask, closed_edge_mask)\n",
        "\n",
        "    # Step 5: Get Transition Mask\n",
        "    erosion_fore_mask = binary_erosion(\n",
        "        fore_mask, structure=np.ones((s_size, s_size)))\n",
        "    dilated_fore_mask = binary_dilation(\n",
        "        fore_mask, structure=np.ones((s_size, s_size)))\n",
        "    transition_mask = np.logical_and(\n",
        "        dilated_fore_mask, np.logical_not(erosion_fore_mask))\n",
        "\n",
        "    # plt.imshow(fore_mask)\n",
        "\n",
        "    # Add more transition_mask based on closed_edge_mask\n",
        "    transition_mask = np.logical_or(transition_mask, closed_edge_mask)\n",
        "\n",
        "    backbone = binary_erosion(\n",
        "        closed_edge_mask, structure=np.ones((s_size+1, s_size+1)))\n",
        "    # plt.imshow(backbone)\n",
        "\n",
        "    absolute_foreground = img[:, :, 3] == 255\n",
        "\n",
        "    backbone = np.logical_or(absolute_foreground, backbone)\n",
        "\n",
        "    f = (img[:, :, 3] < 250) & (img[:, :, 3] > 50)\n",
        "\n",
        "    # Create the trimap\n",
        "    trimap = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
        "    trimap[fore_mask] = 255\n",
        "    trimap[transition_mask] = 127\n",
        "    trimap[f] = 127\n",
        "    trimap[backbone] = 255\n",
        "\n",
        "    trimap[aim_trimap == 0] = 0\n",
        "    mask1 = aim_trimap == 127\n",
        "    mask2 = trimap == 0\n",
        "    mask3 = np.logical_and(mask1, mask2)\n",
        "    trimap[mask3] = 0\n",
        "\n",
        "    # saveMask(fore_mask, \"fore.jpg\")\n",
        "    # saveMask(transition_mask, \"transition_mask.jpg\")\n",
        "    # saveMask(backbone, \"backbone.jpg\")\n",
        "    # saveMask(f, \"fa.jpg\")\n",
        "\n",
        "    return trimap.astype(np.uint8)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def testPostProcessing(file_name, sam_mask):\n",
        "\n",
        "    # Extract the base name without the extension\n",
        "    file_name_origin = os.path.splitext(file_name)[0]\n",
        "\n",
        "    image_path = 'datasets/AIM-500/original/'+file_name_origin+\".jpg\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # RGB numpy array to RGBA numpy array\n",
        "    rgba_image = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
        "    rgba_image[:, :, :3] = image\n",
        "    rgba_image[:, :, 3] = 255\n",
        "\n",
        "    blank_image = np.zeros(\n",
        "        (rgba_image.shape[0], rgba_image.shape[1], 4), dtype=np.uint8)\n",
        "\n",
        "    OUTPUT_FOLDER = \"output/\"\n",
        "\n",
        "    index = file_name_origin\n",
        "\n",
        "    evaluation = {\n",
        "        \"sam\": 0,\n",
        "\n",
        "        \"InSpyReNet\": 0,\n",
        "        \"InSpyReNet_MatteFormer\": 0,\n",
        "\n",
        "        \"AIM\": 0,\n",
        "        \"AIM_MatteFormer\": 0,\n",
        "    }\n",
        "\n",
        "    # sam_mask = masks[index][\"segmentation\"]\n",
        "\n",
        "    IDX_IMG = OUTPUT_FOLDER+str(index)\n",
        "\n",
        "    applied_mask = rgba_image.copy()\n",
        "    applied_mask[sam_mask == False] = [0, 0, 0, 0]\n",
        "\n",
        "    # Crop aplied transparent region (aplied mask)\n",
        "    [PIL_image, bounding] = cropUnusedBlankPixelExtended(\n",
        "        Image.fromarray(applied_mask))\n",
        "\n",
        "    # Crop original image\n",
        "    croped = crop_image_with_bounding_box(rgba_image.copy(), bounding)\n",
        "\n",
        "    # Evaluate ori vs SAM\n",
        "    origin_img = getImage(file_name_origin+\".png\", \"mask\")\n",
        "    origin_alpha = rgbToAlpha2D(origin_img)\n",
        "    origin_mask = origin_img[:, :,\n",
        "                             0] > 0 if origin_img.ndim > 2 else origin_img > 0\n",
        "\n",
        "    # Save origin image\n",
        "    origin_dataset = rgba_image.copy()\n",
        "    origin_dataset[:, :, 3] = origin_alpha\n",
        "\n",
        "    # Evaluate SAM before post processing\n",
        "    res = getMSEandSAD(applied_mask, origin_alpha)\n",
        "    res[\"iou\"] = getIoU(origin_mask, sam_mask)\n",
        "    evaluation[\"sam\"] = res\n",
        "\n",
        "    Image.fromarray(origin_dataset).save(IDX_IMG+\"_origin_dataset.png\")\n",
        "    PIL_image = Image.fromarray(applied_mask)\n",
        "    PIL_image.save(IDX_IMG+\"_origin_sam_applied.png\")\n",
        "    # PIL_image.save(IDX_IMG+\"_origin_mask.png\")\n",
        "\n",
        "    # Save original mask\n",
        "    saveMask(sam_mask, IDX_IMG+\"_sam.jpg\")\n",
        "\n",
        "    # Croped original image to JPG for input AIM and  InSpyReNet\n",
        "    JPG_PATH = IDX_IMG+\"_origin.jpg\"\n",
        "    PIL_image = Image.fromarray(croped)\n",
        "    PIL_image = PIL_image.convert(\"RGB\")\n",
        "    PIL_image.save(JPG_PATH)\n",
        "\n",
        "    # POST PROCESSING PIPELINE\n",
        "    # Remove all region  outside dilated mask\n",
        "    cropped_mask = cropToBoundingBox(sam_mask, bounding)\n",
        "    dilated_sam_mask = binary_dilation(cropped_mask)\n",
        "\n",
        "    # AIM NET _________________________________________\n",
        "    predict, res = AIM_Pipeline(\n",
        "        IDX_IMG, PIL_image, dilated_sam_mask, croped, blank_image, bounding, origin_alpha, origin_mask)\n",
        "\n",
        "    evaluation[\"AIM\"] = res\n",
        "\n",
        "    if (not os.path.exists(IDX_IMG+\"_trimap_aim.jpg\")):\n",
        "        trimap = np.zeros_like(predict)\n",
        "        trimap[predict == 0] = 0  # Background\n",
        "        trimap[predict == 255] = 255  # Foreground\n",
        "        trimap[(predict > 0) & (predict < 255)] = 128  # Unknown region\n",
        "\n",
        "        Image.fromarray(trimap).save(IDX_IMG+\"_trimap_aim.jpg\")\n",
        "    else:\n",
        "        trimap = Image.open(IDX_IMG+\"_trimap_aim.jpg\")\n",
        "        trimap = np.asarray(trimap)\n",
        "\n",
        "    res = Matteformer_Pipeline(\n",
        "        IDX_IMG, \"aim\", JPG_PATH, croped, blank_image, bounding, origin_alpha, origin_mask)\n",
        "\n",
        "    evaluation[\"AIM_MatteFormer\"] = res\n",
        "\n",
        "    # Dichotomous Image Segmentation ______________________\n",
        "\n",
        "    # InSpyReNet\n",
        "    InSpyReNet_result, res = InSpyReNet_Pipeline(\n",
        "        IDX_IMG, JPG_PATH, dilated_sam_mask, blank_image, bounding, origin_alpha, origin_mask)\n",
        "\n",
        "    evaluation[\"InSpyReNet\"] = res\n",
        "\n",
        "    # Image Matting Process _______________________________\n",
        "\n",
        "    # Trimap\n",
        "    if (not os.path.exists(IDX_IMG+\"_trimap_InSpyReNet.jpg\")):\n",
        "        t = makeTrimap(InSpyReNet_result)\n",
        "        Image.fromarray(t).save(IDX_IMG+\"_trimap_InSpyReNet.jpg\")\n",
        "\n",
        "    res = Matteformer_Pipeline(\n",
        "        IDX_IMG, \"InSpyReNet\", JPG_PATH, croped, blank_image, bounding, origin_alpha, origin_mask)\n",
        "    evaluation[\"InSpyReNet_MatteFormer\"] = res\n",
        "\n",
        "\n",
        "    # Trimap\n",
        "    if (not os.path.exists(IDX_IMG+\"_trimap_aim_InSpyReNet.jpg\")):\n",
        "        t = makeTrimapPlusAIM(InSpyReNet_result, trimap)\n",
        "        Image.fromarray(t).save(IDX_IMG+\"_trimap_aim_InSpyReNet.jpg\")\n",
        "\n",
        "    res = Matteformer_Pipeline(\n",
        "        IDX_IMG, \"aim_InSpyReNet\", JPG_PATH, croped, blank_image, bounding, origin_alpha, origin_mask)\n",
        "    evaluation[\"InSpyReNet_AIM_MatteFormer\"] = res\n",
        "\n",
        "    return evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatEvaluation(arrEval):\n",
        "    for i in range(0, len(arrEval)):\n",
        "        print(\"Eval \", i)\n",
        "        for key, value in arrEval[i].items():\n",
        "            print(key, end=\" | \")\n",
        "\n",
        "            # Check if variable is an integer\n",
        "            if isinstance(value, int):\n",
        "                print(\"No value\")\n",
        "            else:\n",
        "                for ev, v in value.items():\n",
        "                    formatted_number = \"{:.3f}\".format(v)\n",
        "                    print(ev + ':', formatted_number, end=\" | \")\n",
        "\n",
        "            print(\"\\n\")\n",
        "\n",
        "        print(\"\\n\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def saveEvalResult(my_dict):\n",
        "    # Get the current date, hour, and minute\n",
        "    current_time = datetime.datetime.now()\n",
        "    formatted_time = current_time.strftime(\"%Y-%m-%d_%H-%M\")\n",
        "\n",
        "    # Generate the filename with date, hour, and minute\n",
        "    filename = f\"eval_result_{formatted_time}.pickle\"\n",
        "\n",
        "    # Save the dictionary to the generated filename\n",
        "    with open(\"evaluations/\"+filename, \"wb\") as file:\n",
        "        pickle.dump(my_dict, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def openEvalResult(file_name):\n",
        "    # Open the pickle file\n",
        "    with open(file_name, \"rb\") as file:\n",
        "        loaded_dict = pickle.load(file)\n",
        "\n",
        "    return loaded_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def printEvaluation(sum_dict):\n",
        "    for key, value in sum_dict.items():\n",
        "        if isinstance(value, int):\n",
        "            print(\"Evaluation from \", value, \" data\")\n",
        "        else:\n",
        "            print(key, end=\" | \")\n",
        "            for ky, val in value.items():\n",
        "                formatted_number = \"{:.3f}\".format(sum_dict[key][ky])\n",
        "                print(ky + ':', formatted_number, end=\" | \")\n",
        "\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatEvaluationMean(arrEval):\n",
        "    length = len(arrEval)\n",
        "    # Get all key\n",
        "    sum_dict = {\n",
        "        \"length\": length\n",
        "    }\n",
        "\n",
        "    for i in range(0, 1):\n",
        "        for key, value in arrEval[i].items():\n",
        "            sum_dict[key] = {\n",
        "                \"sad\": 0,\n",
        "                \"mse\": 0,\n",
        "                \"mad\": 0,\n",
        "                \"conn\": 0,\n",
        "                \"grad\": 0,\n",
        "                \"iou\": 0,\n",
        "                \"time\": 0\n",
        "            }\n",
        "\n",
        "    # print(sum_dict)\n",
        "\n",
        "    for i in range(0, len(arrEval)):\n",
        "        for key, value in arrEval[i].items():\n",
        "            # Check if variable is an integer\n",
        "            if not isinstance(value, int):\n",
        "                for ev, v in value.items():\n",
        "                    sum_dict[key][ev] = sum_dict[key][ev]+v\n",
        "\n",
        "    # print(sum_dict)\n",
        "    for key, value in sum_dict.items():\n",
        "        if not isinstance(value, int):\n",
        "            for ky, val in value.items():\n",
        "                sum_dict[key][ky] = sum_dict[key][ky]/length\n",
        "\n",
        "    # print(sum_dict)\n",
        "    # print(\"Evaluation from \", length, \" data\")\n",
        "\n",
        "    # PRINT\n",
        "    printEvaluation(sum_dict)\n",
        "\n",
        "    # saveEvalResult(sum_dict)\n",
        "    return sum_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "arrEval = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb Cell 40\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m mask_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(PIL_image)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m mask \u001b[39m=\u001b[39m mask_np \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m res \u001b[39m=\u001b[39m testPostProcessing(file_name, mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m arrEval\u001b[39m.\u001b[39mappend(res)\n",
            "\u001b[1;32m/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb Cell 40\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m PIL_image\u001b[39m.\u001b[39msave(IDX_IMG\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_origin_sam_applied.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# PIL_image.save(IDX_IMG+\"_origin_mask.png\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Save original mask\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m saveMask(sam_mask, IDX_IMG\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_sam.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Croped original image to JPG for input AIM and  InSpyReNet\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/susanto/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/prepare.ipynb#X52sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m JPG_PATH \u001b[39m=\u001b[39m IDX_IMG\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_origin.jpg\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[0;32m~/Documents/Proyek/best-remove-background/latihan-remove-background/SegmentAnything/helper/post_helper.py:57\u001b[0m, in \u001b[0;36msaveMask\u001b[0;34m(mask, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msaveMask\u001b[39m(mask, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexample.jpg\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Convert the binary mask to grayscale image\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     gray_image \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n\u001b[0;32m---> 57\u001b[0m     Image\u001b[39m.\u001b[39;49mfromarray(gray_image)\u001b[39m.\u001b[39;49msave(name)\n",
            "File \u001b[0;32m~/miniforge3/envs/coba_torch/lib/python3.9/site-packages/PIL/Image.py:2320\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2317\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2319\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2320\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[1;32m   2321\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   2322\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
            "File \u001b[0;32m~/miniforge3/envs/coba_torch/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:783\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39m# The EXIF info needs to be written as one block, + APP1, + one spare byte.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m# Ensure that our buffer is big enough. Same with the icc_profile block.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m bufsize \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(ImageFile\u001b[39m.\u001b[39mMAXBLOCK, bufsize, \u001b[39mlen\u001b[39m(exif) \u001b[39m+\u001b[39m \u001b[39m5\u001b[39m, \u001b[39mlen\u001b[39m(extra) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 783\u001b[0m ImageFile\u001b[39m.\u001b[39;49m_save(im, fp, [(\u001b[39m\"\u001b[39;49m\u001b[39mjpeg\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)], bufsize)\n",
            "File \u001b[0;32m~/miniforge3/envs/coba_torch/lib/python3.9/site-packages/PIL/ImageFile.py:524\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m         \u001b[39m# slight speedup: compress to real file object\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m         s \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode_to_file(fh, bufsize)\n\u001b[1;32m    525\u001b[0m \u001b[39mif\u001b[39;00m s \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mencoder error \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m when writing image file\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ORIGINAL_PATH = \"precompute/sam_mask\"\n",
        "img_list = os.listdir(ORIGINAL_PATH)\n",
        "# print(img_list)\n",
        "\n",
        "for i in range(0, 5):\n",
        "    print(i)\n",
        "    file_name = img_list[i]\n",
        "    file_name_origin = os.path.splitext(file_name)[0]\n",
        "\n",
        "    # if (os.path.exists(\"output/\"+file_name_origin+\"_origin_dataset.png\") == False):\n",
        "    PIL_image = Image.open(ORIGINAL_PATH+\"/\"+file_name)\n",
        "\n",
        "    mask_np = np.asarray(PIL_image)\n",
        "\n",
        "    mask = mask_np > 0\n",
        "\n",
        "    res = testPostProcessing(file_name, mask)\n",
        "\n",
        "    arrEval.append(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval  0\n",
            "sam | sad: 75.281 | mse: 0.030 | mad: 0.036 | conn: 76.616 | grad: 142.766 | iou: 0.895 | \n",
            "\n",
            "InSpyReNet | sad: 41.405 | mse: 0.009 | mad: 0.020 | conn: 39.199 | grad: 48.499 | iou: 0.911 | time: 4.238 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 15.445 | mse: 0.002 | mad: 0.007 | conn: 13.315 | grad: 14.088 | iou: 0.925 | time: 11.947 | \n",
            "\n",
            "AIM | sad: 27.457 | mse: 0.006 | mad: 0.013 | conn: 25.108 | grad: 45.445 | iou: 0.912 | time: 7.719 | \n",
            "\n",
            "AIM_MatteFormer | sad: 21.640 | mse: 0.005 | mad: 0.010 | conn: 19.211 | grad: 33.486 | iou: 0.912 | time: 12.325 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eval  1\n",
            "sam | sad: 42.521 | mse: 0.020 | mad: 0.024 | conn: 43.442 | grad: 111.873 | iou: 0.966 | \n",
            "\n",
            "InSpyReNet | sad: 30.727 | mse: 0.008 | mad: 0.018 | conn: 26.157 | grad: 49.494 | iou: 0.975 | time: 3.091 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 15.485 | mse: 0.004 | mad: 0.009 | conn: 14.580 | grad: 15.544 | iou: 0.986 | time: 9.703 | \n",
            "\n",
            "AIM | sad: 9.626 | mse: 0.002 | mad: 0.006 | conn: 8.762 | grad: 8.057 | iou: 0.991 | time: 4.882 | \n",
            "\n",
            "AIM_MatteFormer | sad: 9.896 | mse: 0.002 | mad: 0.006 | conn: 8.869 | grad: 8.171 | iou: 0.988 | time: 9.083 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eval  2\n",
            "sam | sad: 1510.720 | mse: 0.950 | mad: 0.972 | conn: 1538.081 | grad: 71.626 | iou: 0.989 | \n",
            "\n",
            "InSpyReNet | sad: 43.513 | mse: 0.006 | mad: 0.028 | conn: 16.168 | grad: 71.603 | iou: 0.010 | time: 2.014 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 43.400 | mse: 0.006 | mad: 0.028 | conn: 0.000 | grad: 71.626 | iou: 0.000 | time: 7.293 | \n",
            "\n",
            "AIM | sad: 41.587 | mse: 0.005 | mad: 0.027 | conn: 14.984 | grad: 56.972 | iou: 0.031 | time: 3.825 | \n",
            "\n",
            "AIM_MatteFormer | sad: 41.156 | mse: 0.005 | mad: 0.026 | conn: 13.976 | grad: 55.216 | iou: 0.008 | time: 8.674 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eval  3\n",
            "sam | sad: 75.281 | mse: 0.030 | mad: 0.036 | conn: 76.616 | grad: 142.766 | iou: 0.895 | \n",
            "\n",
            "InSpyReNet | sad: 41.405 | mse: 0.009 | mad: 0.020 | conn: 39.199 | grad: 48.499 | iou: 0.911 | time: 4.387 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 16.439 | mse: 0.003 | mad: 0.008 | conn: 14.394 | grad: 14.257 | iou: 0.922 | time: 11.679 | \n",
            "\n",
            "AIM | sad: 27.457 | mse: 0.006 | mad: 0.013 | conn: 25.108 | grad: 45.445 | iou: 0.912 | time: 7.421 | \n",
            "\n",
            "AIM_MatteFormer | sad: 21.640 | mse: 0.005 | mad: 0.010 | conn: 19.211 | grad: 33.486 | iou: 0.912 | time: 13.274 | \n",
            "\n",
            "InSpyReNet_AIM_MatteFormer | sad: 16.439 | mse: 0.003 | mad: 0.008 | conn: 14.394 | grad: 14.257 | iou: 0.922 | time: 11.235 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eval  4\n",
            "sam | sad: 42.521 | mse: 0.020 | mad: 0.024 | conn: 43.442 | grad: 111.873 | iou: 0.966 | \n",
            "\n",
            "InSpyReNet | sad: 30.727 | mse: 0.008 | mad: 0.018 | conn: 26.157 | grad: 49.494 | iou: 0.975 | time: 3.699 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 15.485 | mse: 0.004 | mad: 0.009 | conn: 14.580 | grad: 15.544 | iou: 0.986 | time: 10.094 | \n",
            "\n",
            "AIM | sad: 9.626 | mse: 0.002 | mad: 0.006 | conn: 8.762 | grad: 8.057 | iou: 0.991 | time: 5.667 | \n",
            "\n",
            "AIM_MatteFormer | sad: 9.896 | mse: 0.002 | mad: 0.006 | conn: 8.869 | grad: 8.171 | iou: 0.988 | time: 10.727 | \n",
            "\n",
            "InSpyReNet_AIM_MatteFormer | sad: 15.485 | mse: 0.004 | mad: 0.009 | conn: 14.580 | grad: 15.544 | iou: 0.986 | time: 9.470 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eval  5\n",
            "sam | sad: 1510.720 | mse: 0.950 | mad: 0.972 | conn: 1538.081 | grad: 71.626 | iou: 0.989 | \n",
            "\n",
            "InSpyReNet | sad: 43.513 | mse: 0.006 | mad: 0.028 | conn: 16.168 | grad: 71.603 | iou: 0.010 | time: 2.016 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 43.400 | mse: 0.006 | mad: 0.028 | conn: 0.000 | grad: 71.626 | iou: 0.000 | time: 7.234 | \n",
            "\n",
            "AIM | sad: 41.587 | mse: 0.005 | mad: 0.027 | conn: 14.984 | grad: 56.972 | iou: 0.031 | time: 3.825 | \n",
            "\n",
            "AIM_MatteFormer | sad: 41.156 | mse: 0.005 | mad: 0.026 | conn: 13.976 | grad: 55.216 | iou: 0.008 | time: 7.503 | \n",
            "\n",
            "InSpyReNet_AIM_MatteFormer | sad: 43.400 | mse: 0.006 | mad: 0.028 | conn: 0.000 | grad: 71.626 | iou: 0.000 | time: 6.981 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eval  6\n",
            "sam | sad: 56.170 | mse: 0.029 | mad: 0.032 | conn: 57.324 | grad: 134.875 | iou: 0.915 | \n",
            "\n",
            "InSpyReNet | sad: 29.655 | mse: 0.008 | mad: 0.017 | conn: 26.451 | grad: 55.417 | iou: 0.943 | time: 2.747 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 8.489 | mse: 0.002 | mad: 0.005 | conn: 7.601 | grad: 9.976 | iou: 0.959 | time: 9.935 | \n",
            "\n",
            "AIM | sad: 10.527 | mse: 0.002 | mad: 0.006 | conn: 9.868 | grad: 11.534 | iou: 0.963 | time: 5.271 | \n",
            "\n",
            "AIM_MatteFormer | sad: 10.719 | mse: 0.002 | mad: 0.006 | conn: 9.942 | grad: 13.850 | iou: 0.960 | time: 9.597 | \n",
            "\n",
            "InSpyReNet_AIM_MatteFormer | sad: 8.489 | mse: 0.002 | mad: 0.005 | conn: 7.601 | grad: 9.976 | iou: 0.959 | time: 9.429 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eval  7\n",
            "sam | sad: 35.772 | mse: 0.018 | mad: 0.020 | conn: 35.881 | grad: 137.033 | iou: 0.924 | \n",
            "\n",
            "InSpyReNet | sad: 32.544 | mse: 0.012 | mad: 0.019 | conn: 29.212 | grad: 96.584 | iou: 0.921 | time: 3.289 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 11.384 | mse: 0.003 | mad: 0.006 | conn: 10.177 | grad: 27.582 | iou: 0.948 | time: 11.890 | \n",
            "\n",
            "AIM | sad: 9.942 | mse: 0.002 | mad: 0.006 | conn: 8.491 | grad: 20.377 | iou: 0.947 | time: 5.052 | \n",
            "\n",
            "AIM_MatteFormer | sad: 12.017 | mse: 0.003 | mad: 0.007 | conn: 10.730 | grad: 26.771 | iou: 0.947 | time: 10.458 | \n",
            "\n",
            "InSpyReNet_AIM_MatteFormer | sad: 11.384 | mse: 0.003 | mad: 0.006 | conn: 10.177 | grad: 27.582 | iou: 0.948 | time: 9.268 | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# formatEvaluation(arrEval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation from  5  data\n",
            "\n",
            "\n",
            "sam | sad: 344.093 | mse: 0.209 | mad: 0.217 | conn: 350.269 | grad: 119.635 | iou: 0.938 | time: 0.000 | \n",
            "\n",
            "InSpyReNet | sad: 35.569 | mse: 0.009 | mad: 0.020 | conn: 27.437 | grad: 64.319 | iou: 0.752 | time: 3.102 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 19.039 | mse: 0.004 | mad: 0.011 | conn: 9.350 | grad: 27.797 | iou: 0.763 | time: 9.733 | \n",
            "\n",
            "AIM | sad: 19.828 | mse: 0.004 | mad: 0.011 | conn: 13.443 | grad: 28.477 | iou: 0.769 | time: 5.732 | \n",
            "\n",
            "AIM_MatteFormer | sad: 19.086 | mse: 0.004 | mad: 0.011 | conn: 12.546 | grad: 27.499 | iou: 0.763 | time: 9.829 | \n",
            "\n",
            "InSpyReNet_AIM_MatteFormer | sad: 19.039 | mse: 0.004 | mad: 0.011 | conn: 9.350 | grad: 27.797 | iou: 0.763 | time: 10.068 | \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sum_dict = formatEvaluationMean(arrEval)\n",
        "\n",
        "# saveEvalResult(sum_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation from  500  data\n",
            "\n",
            "\n",
            "sam | sad: 80.105 | mse: 0.042 | mad: 0.049 | conn: 82.025 | grad: 97.745 | iou: 0.915 | time: 0.000 | \n",
            "\n",
            "InSpyReNet | sad: 60.857 | mse: 0.025 | mad: 0.036 | conn: 60.848 | grad: 107.096 | iou: 0.915 | time: 2.430 | \n",
            "\n",
            "InSpyReNet_MatteFormer | sad: 39.920 | mse: 0.016 | mad: 0.024 | conn: 39.368 | grad: 53.031 | iou: 0.912 | time: 3.032 | \n",
            "\n",
            "AIM | sad: 41.856 | mse: 0.016 | mad: 0.025 | conn: 39.805 | grad: 36.662 | iou: 0.900 | time: 2.983 | \n",
            "\n",
            "AIM_MatteFormer | sad: 55.593 | mse: 0.024 | mad: 0.033 | conn: 53.270 | grad: 50.751 | iou: 0.888 | time: 2.971 | \n",
            "\n"
          ]
        }
      ],
      "source": [
        "a = openEvalResult(\"evaluations/eval-500_result_2023-06-02_12-34.pickle\")\n",
        "\n",
        "# print(a)\n",
        "# formatEvaluation(a)\n",
        "printEvaluation(a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "coba_torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
